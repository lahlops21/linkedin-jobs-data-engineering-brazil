id: linkedin_jobs_pipeline
namespace: linkedin

description: |
  Ingestion pipeline for LinkedIn jobs dataset

variables:
  file: "engenheiro_de_dados_6k.csv"
  gcs_file: "gs://project-linkedin-datalake-2026/raw/{{vars.file}}"

tasks:

  - id: upload_to_gcs
    type: io.kestra.plugin.gcp.gcs.Upload
    from: /app/data/{{vars.file}}"
    to: "{{vars.gcs_file}}"

  - id: load_to_bigquery
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
      LOAD DATA INTO
      `linkedin-data-pipeline-project.project_linkedin_dataset_2026.jobs_raw`
      FROM FILES (
        format = 'CSV',
        uris = ['{{vars.gcs_file}}'],
        skip_leading_rows = 1
      );

  - id: build_dw
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
      CREATE OR REPLACE TABLE
      `linkedin-data-pipeline-project.project_linkedin_dataset_2026.jobs_dw`
      PARTITION BY created_date
      CLUSTER BY company_name, location, experience_level
      AS
      SELECT
        job_id,
        COALESCE(company_name,'Unknown') company_name,
        location,
        standardized_title,
        COALESCE(formatted_experience_level,'Not informed') experience_level,
        formatted_employment_status,
        min_salary,
        max_salary,
        created_date
      FROM
      `linkedin-data-pipeline-project.project_linkedin_dataset_2026.jobs_clean_v2`;
